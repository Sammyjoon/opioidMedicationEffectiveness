{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from pymetamap import MetaMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the complete list od semantic types from the text file and make a dictionary\n",
    "semTypesList = pd.read_csv('SemanticTypes_2013AA.txt', sep='|')\n",
    "semList = semTypesList.set_index('shortSem').to_dict()['LongSem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Normalization Fuctions\n",
    "# Tokenization\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = str(row['review'])\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "\n",
    "# Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "def stem_list(row):\n",
    "    my_list = row['words']\n",
    "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
    "    return (stemmed_list)\n",
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['words']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "\n",
    "# Re-join words\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['stem_meaningful']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "# Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "file = '/Users/sammie/JMIR_p1/All_Data_Drugs.com&WebMD_Methadone&Suboxone.xlsx'\n",
    "\n",
    "raw_df = pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(raw_df))\n",
    "raw_df.iloc[0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MetaMap.get_instance('/Users/sammie/JMIR_p1/public_mm/bin/metamap18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing reviews\n",
    "\n",
    "raw_df['review'] = raw_df['review'].str.lower()\n",
    "\n",
    "raw_df['words'] = raw_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "# raw_df['stemmed_words'] = raw_df.apply(stem_list, axis=1)\n",
    "\n",
    "raw_df['stem_meaningful'] = raw_df.apply(remove_stops, axis=1)\n",
    "\n",
    "raw_df['processed'] = raw_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "\n",
    "# Remove non-Ascii\n",
    "\n",
    "from string import printable\n",
    "\n",
    "st = set(printable)\n",
    "raw_df[\"processed\"] = raw_df[\"processed\"].apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns = ['Normalized Text','Identified Phrase','Mapped Standard Phrase', 'Semantic Types' , 'Short Semantic Types'])\n",
    "df2 = pd.DataFrame(columns = ['Normalized Text','Identified Phrase','Mapped Standard Phrase', 'Semantic Types' , 'Short Semantic Types'])\n",
    "df3 = pd.DataFrame(columns = ['Normalized Text','Identified Phrase','Mapped Standard Phrase', 'Semantic Types' , 'Short Semantic Types'])\n",
    "# df4 = pd.DataFrame(columns = ['Normalized Text','Identified Phrase','Mapped Standard Phrase', 'Semantic Types' , 'Short Semantic Types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (len(raw_df))\n",
    "\n",
    "for indx in range (len(raw_df)):\n",
    "    print(indx)\n",
    "    review = raw_df.iloc[indx]['processed']\n",
    "    concepts,error = mm.extract_mapping_candidates([review], mm_data_version='USAbase')\n",
    "    if error is None:\n",
    "        j = concepts.find('{\"AllDocuments\"')\n",
    "        data = json.loads(concepts[j:])\n",
    "        for document in data['AllDocuments']:\n",
    "            for utter in document['Document']['Utterances']:\n",
    "                for phrase in utter['Phrases']:\n",
    "                    # print(phrase)\n",
    "                    for mapping in phrase['Mappings']:\n",
    "                        for mappingCandidate in mapping['MappingCandidates']:\n",
    "                            semTypes = mappingCandidate['SemTypes']\n",
    "\n",
    "#                             if 'sosy' in semTypes:\n",
    "#                                 if not any(df['Normalized Text'] == phrase['PhraseText']):\n",
    "#                                     df = df.append({'Normalized Text':phrase['PhraseText'],'Short Semantic Types':semTypes, \\\n",
    "#                                         'Identified Phrase': mappingCandidate['CandidateMatched'], \\\n",
    "#                                         'Semantic Types' : ', '.join([semList[x] for x in semTypes]) , \\\n",
    "#                                         'Mapped Standard Phrase': mappingCandidate['CandidatePreferred']}, ignore_index = True)\n",
    "                            if 'mobd' in semTypes:\n",
    "                                if not any(df1['Normalized Text'] == phrase['PhraseText']):\n",
    "                                    df1 = df1.append({'Normalized Text':phrase['PhraseText'],'Short Semantic Types':semTypes, \\\n",
    "                                        'Identified Phrase': mappingCandidate['CandidateMatched'], \\\n",
    "                                        'Semantic Types' : ', '.join([semList[x] for x in semTypes]) , \\\n",
    "                                        'Mapped Standard Phrase': mappingCandidate['CandidatePreferred']}, ignore_index = True)\n",
    "\n",
    "                            if 'dsyn' in semTypes:\n",
    "                                if not any(df2['Normalized Text'] == phrase['PhraseText']):\n",
    "                                    df2 = df2.append({'Normalized Text':phrase['PhraseText'],'Short Semantic Types':semTypes, \\\n",
    "                                        'Identified Phrase': mappingCandidate['CandidateMatched'], \\\n",
    "                                        'Semantic Types' : ', '.join([semList[x] for x in semTypes]) , \\\n",
    "                                        'Mapped Standard Phrase': mappingCandidate['CandidatePreferred']}, ignore_index = True)\n",
    "\n",
    "                            if 'orch' in semTypes or 'clnd' in semTypes:\n",
    "                                if not any(df3['Normalized Text'] == phrase['PhraseText']):\n",
    "                                    df3 = df3.append({'Normalized Text':phrase['PhraseText'],'Short Semantic Types':semTypes, \\\n",
    "                                        'Identified Phrase': mappingCandidate['CandidateMatched'], \\\n",
    "                                        'Semantic Types' : ', '.join([semList[x] for x in semTypes]) , \\\n",
    "                                        'Mapped Standard Phrase': mappingCandidate['CandidatePreferred']}, ignore_index = True)\n",
    "\n",
    "    else:\n",
    "        print('error in', indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('/Users/sammie/JMIR_p1/MetaMap_Results/result_mobd.csv')\n",
    "df2.to_csv('/Users/sammie/JMIR_p1/MetaMap_Results/result_dsyn.csv')\n",
    "df3.to_csv('/Users/sammie/JMIR_p1/MetaMap_Results/result_orch_clnd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37764bitanaconda3virtualenv89cd8e19bc9946e88ffeaa66bd2d4218"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
